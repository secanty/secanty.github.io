<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>适用于 Windows 的 Adobe 全家桶激活工具</title>
      <link href="/posts/2.html"/>
      <url>/posts/2.html</url>
      
        <content type="html"><![CDATA[<p><span style="font-size: 2em; display: block; text-align: center; color: rgba(255,255,255,0.9); font-weight: bold;"><span class='p anzhiyufont anzhiyu-icon-fan red h1'></span> 前言</span></p><p>推荐一个适用于<a href="#pre">适用于 Windows 的 Adobe 全家桶激活工具</a>，截至 <strong>2024 年 8 月 12 日</strong>，激活工具可用于激活<a href="#notification">当前最新版本</a>（具体版本号可在文末注意事项中查看），其他版本自行测试。下面，将以激活 Photoshop 为例演示如何使用该工具进行激活 Adobe 全家桶。</p><p><strong>请注意</strong>：本教程<strong>仅供学习交流使用</strong>，请支持正版软件，遵守法律法规。</p><h2 id="准备工作"><span id="pre">准备工作</span></h2><ol type="1"><li><strong>关闭杀毒软件</strong>：激活工具会被识别成恶意程序，因此需要先关闭杀毒软件，或者把工具目录添加到免杀目录中。</li><li><strong>从官方渠道安装 Photoshop</strong> ：官方不直接提供 Photoshop 的安装包，需要先行安装 <a href="https://creativecloud.adobe.com/apps/download/creative-cloud"><strong>Creative Cloud Desktop</strong></a>，然后通过 Creative Cloud Desktop 安装 Photoshop，其默认安装位置为 C 盘，如有需要可依次点击 <em>头像 -&gt; 首选项 -&gt; 应用程序 -&gt; 安装位置</em> 进行修改。</li><li><strong>下载激活工具</strong>： <a class="btn-anzhiyu orange" href="https://www.aspark.cc/file/AdobeGenP.zip"   title="点击下载"><i class="anzhiyufont anzhiyu-icon-circle-arrow-right"></i><span>点击下载</span></a></li></ol><h2 id="未完待续......">未完，待续......</h2><h2 id="注意事项"><span id="notification">注意事项</span></h2><ol type="1"><li><p><strong>教程具有时效性</strong>：Adobe 会不断更新其软件和验证机制，不保证对最新版本有效，如果激活失败，可以尝试老版本。</p></li><li><p><strong>尽量不要更新 Adobe 软件</strong>：更新会导致激活失效，此时可以尝试重新运行激活工具，如果激活失败，请自行回退版本。</p></li><li><p><strong>风险提示</strong>：使用激活工具存在一定的法律风险，可能会导致软件无法正常使用或面临法律纠纷。请权衡利弊，慎重选择。</p></li><li><p><strong>经测试可正常激活的版本</strong>: 下面给出截至发布日经过测试可以正常激活的版本，其他版本自行测试。</p></li></ol><div class="note info simple"><ul><li><strong>Photoshop</strong>: 25.11</li><li><strong>Premiere Pro</strong>: 24.5</li><li><strong>After Effects</strong>: 24.5</li><li><strong>Acrobat</strong>: 24.1</li><li><strong>Animate</strong>: 24.0.4</li><li><strong>Character Animator</strong>: 24.2</li><li><strong>Audition</strong>: 24.4.1</li><li><strong>Media Encoder</strong>: 24.5</li><li><strong>Illustrator</strong>: 28.6</li><li><strong>InDesign</strong>: 19.5</li><li><strong>InCopy</strong>: 19.5</li><li><strong>Dreamweaver</strong>: 21.4</li><li><strong>lightroom</strong>: 7.4.1</li></ul></div>]]></content>
      
      
      
        <tags>
            
            <tag> Adobe </tag>
            
            <tag> 激活工具 </tag>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可解释机器学习 —— LIME 篇</title>
      <link href="/posts/1.html"/>
      <url>/posts/1.html</url>
      
        <content type="html"><![CDATA[<h2 id="一什么是解释">一、什么是解释？</h2><p>所谓解释，指的是使用我们能够理解的方法（如图像、文本、简化的模型等）来揭示输入数据的特定部分（例如文本中的词语、图像中的像素）与模型做出特定预测之间的关系。这种解释帮助我们定性地理解模型的工作原理和决策过程，尤其是对于那些复杂且通常被视为“黑盒”的模型。简而言之，解释旨在建立模型输入与输出之间的直观联系，使得模型的行为对于人类用户更加透明和可理解。</p><h2 id="二我们为什么需要解释">二、我们为什么需要解释？</h2><ol type="1"><li>决定是否该信任某个模型/预测</li><li>在不同模型之间做出选择</li><li>通过特征工程改进模型</li><li>确定某个模型为什么不可信</li></ol><blockquote><p>由此可以很容易知道解释在机器学习领域的重要性，但现如今所使用的很多机器学习模型尽管有着不错的性能，然而在解释性方面却是不足的。在机器学习模型的很多应用中，我们有时候必须要知道模型为什么做出这个预测，以及这个预测是否合理，而LIME正是为解决这个问题而诞生的算法。</p></blockquote><h2 id="三相关论文">三、相关论文</h2><p><a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939778"><em>LIME:</em> Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.</a></p><p><strong><em>论文的主要成果：</em></strong></p><p><em>We propose providing explanations for individual predictions as a solution to the “trusting a prediction” problem, and selecting multiple such predictions (and explanations) as a solution to the “trusting the model” problem.</em></p><p><em>We proposed LIME, a modular and extensible approach to faithfully explain the predictions of any model in an interpretable manner. We also introduced SP-LIME, a method to select representative and non-redundant predictions, providing a global view of the model to users.</em></p><p><strong><em>代码仓库：</em></strong> <a href="https://github.com/marcotcr/lime">Lime: Explaining the predictions of any machine learning classifier - marcotcr/lime</a></p><h2 id="四lime算法">四、LIME算法</h2><h3 id="介绍">1. 介绍</h3><p><strong>LIME</strong>（即Local Interpretable Model-Agnostic Explanations）是一种局部解释的算法，它旨在为分类器或回归器的预测提供解释。通过使用“可解释特征”训练“可解释模型”，在“样本的局部线性邻域”拟合“原模型”，从而实现对复杂模型预测的解释。这种方法有助于增加模型的透明度和可信度，尤其是在需要解释预测以获得用户信任的情况下。</p><blockquote><p>LIME属于事后解释方法，具有局部性、可解释性、与模型无关性的特征。</p></blockquote><h3 id="原理">2. 原理</h3><p>如下图所示，LIME首先在待解释样本点周围进行扰动采样，并按照扰动样本与原样本之间的相似度或距离赋予样本权重，然后用原模型对这些样本进行预测而获得标签值，并使用这些局部样本训练一个可解释性比较强的线性模型，这样就可以利用线性模型的可解释性对复杂模型进行局部解释。</p><p><img src="https://www.aspark.cc/file/202403172154071.png" /></p><p>算法使用目标函数<span class="math inline">\(\xi(x) = \underset{g \in G}{argmin}\ L(f, g, \pi_{x}) + \Omega(g)\)</span>衡量两个模型之间的差异。</p><h3 id="优点">3. 优点</h3><ul><li>模型无关性（通用性强）：LIME能够兼容任何一种机器学习算法，具有广泛的适用性</li><li>可解释单个样本预测结果、选取代表性样本</li><li>LIME是少数适用于表格数据、文本和图像的方法之一</li></ul><h3 id="缺点">缺点</h3><ul><li>局部保真度不意味着全局保真度：在全局上重要的特征在局部上可能并不重要，反之亦然。</li><li>原模型如果在局部仍然非线性，可解释模型难以拟合原模型</li><li>时间成本高：每个待测样本都需训练对应可解释模型，耗时长</li><li>通过扰动得到的样本符合高斯分布，却忽略了特征之间的关系</li></ul><h3 id="实例">4. 实例</h3><p>使用20newsgroups数据集中的alt.atheism和soc.religion.christianlian训练一个RFC模型，并且使用LIME对单个样本进行解释</p><ul><li>对选定的单个样本进行解释，结果如下：（样本的目标值为atheism）</li></ul><p><img src="https://www.aspark.cc/file/202403172154632.png" /></p><ul><li>将上述样本中的headers、footers、quotes移除后，再对该样本进行解释，结果如下：</li></ul><p><img src="https://www.aspark.cc/file/202403172155176.png" /></p><p>通过LIME我们可以很容易看出模型模型做出一个预测依赖于哪些特征，以及根据这些特征特征做出的预测是否合理。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 可解释机器学习 </tag>
            
            <tag> LIME </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
