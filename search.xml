<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>可解释机器学习 —— LIME 篇</title>
      <link href="/posts/1.html"/>
      <url>/posts/1.html</url>
      
        <content type="html"><![CDATA[<h2 id="一什么是解释">一、什么是解释？</h2><p>所谓解释，指的是使用我们能够理解的方法（如图像、文本、简化的模型等）来揭示输入数据的特定部分（例如文本中的词语、图像中的像素）与模型做出特定预测之间的关系。这种解释帮助我们定性地理解模型的工作原理和决策过程，尤其是对于那些复杂且通常被视为“黑盒”的模型。简而言之，解释旨在建立模型输入与输出之间的直观联系，使得模型的行为对于人类用户更加透明和可理解。</p><h2 id="二我们为什么需要解释">二、我们为什么需要解释？</h2><ol type="1"><li>决定是否该信任某个模型/预测</li><li>在不同模型之间做出选择</li><li>通过特征工程改进模型</li><li>确定某个模型为什么不可信</li></ol><blockquote><p>由此可以很容易知道解释在机器学习领域的重要性，但现如今所使用的很多机器学习模型尽管有着不错的性能，然而在解释性方面却是不足的。在机器学习模型的很多应用中，我们有时候必须要知道模型为什么做出这个预测，以及这个预测是否合理，而LIME正是为解决这个问题而诞生的算法。</p></blockquote><h2 id="三相关论文">三、相关论文</h2><p><a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939778"><em>LIME:</em> Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.</a></p><p><strong><em>论文的主要成果：</em></strong></p><p><em>We propose providing explanations for individual predictions as a solution to the “trusting a prediction” problem, and selecting multiple such predictions (and explanations) as a solution to the “trusting the model” problem.</em></p><p><em>We proposed LIME, a modular and extensible approach to faithfully explain the predictions of any model in an interpretable manner. We also introduced SP-LIME, a method to select representative and non-redundant predictions, providing a global view of the model to users.</em></p><p><strong><em>代码仓库：</em></strong> <a href="https://github.com/marcotcr/lime">Lime: Explaining the predictions of any machine learning classifier - marcotcr/lime</a></p><h2 id="四lime算法">四、LIME算法</h2><h3 id="介绍">1. 介绍</h3><p><strong>LIME</strong>（即Local Interpretable Model-Agnostic Explanations）是一种局部解释的算法，它旨在为分类器或回归器的预测提供解释。通过使用“可解释特征”训练“可解释模型”，在“样本的局部线性邻域”拟合“原模型”，从而实现对复杂模型预测的解释。这种方法有助于增加模型的透明度和可信度，尤其是在需要解释预测以获得用户信任的情况下。</p><blockquote><p>LIME属于事后解释方法，具有局部性、可解释性、与模型无关性的特征。</p></blockquote><h3 id="原理">2. 原理</h3><p>如下图所示，LIME首先在待解释样本点周围进行扰动采样，并按照扰动样本与原样本之间的相似度或距离赋予样本权重，然后用原模型对这些样本进行预测而获得标签值，并使用这些局部样本训练一个可解释性比较强的线性模型，这样就可以利用线性模型的可解释性对复杂模型进行局部解释。</p><p><img src="https://www.aspark.cc/file/202403172154071.png" /></p><p>算法使用目标函数<span class="math inline">\(\xi(x) = \underset{g \in G}{argmin}\ L(f, g, \pi_{x}) + \Omega(g)\)</span>衡量两个模型之间的差异。</p><h3 id="优点">3. 优点</h3><ul><li>模型无关性（通用性强）：LIME能够兼容任何一种机器学习算法，具有广泛的适用性</li><li>可解释单个样本预测结果、选取代表性样本</li><li>LIME是少数适用于表格数据、文本和图像的方法之一</li></ul><h3 id="缺点">缺点</h3><ul><li>局部保真度不意味着全局保真度：在全局上重要的特征在局部上可能并不重要，反之亦然。</li><li>原模型如果在局部仍然非线性，可解释模型难以拟合原模型</li><li>时间成本高：每个待测样本都需训练对应可解释模型，耗时长</li><li>通过扰动得到的样本符合高斯分布，却忽略了特征之间的关系</li></ul><h3 id="实例">4. 实例</h3><p>使用20newsgroups数据集中的alt.atheism和soc.religion.christianlian训练一个RFC模型，并且使用LIME对单个样本进行解释</p><ul><li>对选定的单个样本进行解释，结果如下：（样本的目标值为atheism）</li></ul><p><img src="https://www.aspark.cc/file/202403172154632.png" /></p><ul><li>将上述样本中的headers、footers、quotes移除后，再对该样本进行解释，结果如下：</li></ul><p><img src="https://www.aspark.cc/file/202403172155176.png" /></p><p>通过LIME我们可以很容易看出模型模型做出一个预测依赖于哪些特征，以及根据这些特征特征做出的预测是否合理。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 可解释机器学习 </tag>
            
            <tag> LIME </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
